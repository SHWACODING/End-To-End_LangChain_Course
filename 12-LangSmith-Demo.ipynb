{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f099d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8f5433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002454FED7D90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024550FF4050>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2a4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Please resposne to the user request only based on the given context\"),\n",
    "        (\"user\", \"Question:{question}\\nContext:{context}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e7594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611cd49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb2f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Could You Simplify me This Article?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffff998",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm \n",
    "for advancing the reasoning capabilities of Large Language Models (LLMs).\n",
    "However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their \n",
    "base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR \n",
    "merely re-weights existing reasoning paths at the cost of reasoning diversity.\n",
    "In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ \n",
    "metric itself is a flawed measure of reasoning, as it credits correct final answers that probably \n",
    "arise from inaccurate or incomplete chains of thought (CoTs). To address this, \n",
    "we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates \n",
    "that both the reasoning path and the final answer be correct.\n",
    "We provide a new theoretical foundation that formalizes how RLVR, unlike \n",
    "traditional RL, is uniquely structured to incentivize logical integrity.\n",
    "Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can \n",
    "incentivize the generalization of correct reasoning for all values of $K$.\n",
    "Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning\n",
    "capability emerges early in the training process and smoothly generalizes.\n",
    "Our work provides a clear perspective on the role of RLVR, offers a more reliable method \n",
    "for its evaluation, and confirms its potential to genuinely advance machine reasoning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7505b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to simplify the article for you. \\n\\nThe article talks about a way to improve language models called Reinforcement Learning with Verifiable Rewards (RLVR). However, there's a problem: even though RLVR is supposed to make models smarter, they often don't perform as well as the original models in certain tests.\\n\\nThe authors of the article think they've found the reason for this problem. They believe that the way we measure how well the models are doing is flawed. The current measure, called $Pass@K$, only checks if the final answer is correct, but it doesn't care if the model got there using the right reasoning.\\n\\nTo fix this, the authors created a new way to measure how well the models are doing, called $CoT$-$Pass@K$. This measure checks both the final answer and the reasoning behind it.\\n\\nThe authors found that when they used this new measure, the RLVR models actually performed better than the original models. They also found that the RLVR models started to reason better early in the training process and kept getting better.\\n\\nOverall, the article says that RLVR has the potential to make language models smarter and more reasonable, and that we just need to use the right measure to see its true benefits.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a4f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
