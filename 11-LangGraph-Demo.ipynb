{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec201c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e889079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002720CF3B010>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002720E046D90>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b100b5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the Top Reinforcement Learning Papers?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://github.com/Allenpandas/Reinforcement-Learning-Papers',\n",
       "   'title': 'Allenpandas/Reinforcement-Learning-Papers: 📚 List of ...',\n",
       "   'content': 'GitHub - Allenpandas/Reinforcement-Learning-Papers: 📚 List of Top-tier Conference Papers on Reinforcement Learning (RL)，including: NeurIPS, ICML, AAAI, IJCAI, AAMAS, ICLR, ICRA, etc. 📚 List of Top-tier Conference Papers on Reinforcement Learning (RL)，including: NeurIPS, ICML, AAAI, IJCAI, AAMAS, ICLR, ICRA, etc. This repository is dedicated to curating significant research papers in the field of **Reinforcement Learning (RL)** that have been accepted at top academic conferences such as **AAAI**, **IJCAI**, **NeurIPS**, **ICML**, **ICLR**, **ICRA**, **AAMAS** and more. *   Chao Yu, Xinyi Yang, Jiaxuan Gao, Jiayu Chen, Yunfei Li, Jijia Liu, Yunfei Xiang, Ruixin Huang, Huazhong Yang, Yi Wu, Yu Wang. 📚 List of Top-tier Conference Papers on Reinforcement Learning (RL)，including: NeurIPS, ICML, AAAI, IJCAI, AAMAS, ICLR, ICRA, etc.',\n",
       "   'score': 0.8396422,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.reddit.com/r/reinforcementlearning/comments/w3i4p0/what_are_the_first_5_papers_i_should_read_on_rl/',\n",
       "   'title': 'What are the first 5 papers I should read on RL?',\n",
       "   'content': 'Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al, 2017. Algorithm: Rainbow DQN. · Proximal Policy Optimization ...See more',\n",
       "   'score': 0.63125396,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://spinningup.openai.com/en/latest/spinningup/keypapers.html',\n",
       "   'title': 'Key Papers in Deep RL — Spinning Up documentation',\n",
       "   'content': 'Acknowledgements About the Author Spinning Up Docs » Key Papers in Deep RL Edit on GitHub Key Papers in Deep RL¶ What follows is a list of papers in deep RL that are worth reading. This is far from comprehensive, but should provide a useful starting point for someone looking to do research in the field. Deep Q-Learning¶ [1]Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013. Policy Gradients¶ [7]Asynchronous Methods for Deep Reinforcement Learning, Mnih et al, 2016.',\n",
       "   'score': 0.6271529,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.61}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_search = TavilySearch(max_results=3)\n",
    "\n",
    "tavily_search.invoke({\"query\": \"What is the Top Reinforcement Learning Papers?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f9fcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TavilySearch(max_results=3, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'), api_base_url=None))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [tavily_search]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b386d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09af3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Do you know Reinforcement Learning With Verifiable Rewards Paper, explain it?\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd0b014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Do you know Reinforcement Learning With Verifiable Rewards Paper, explain it?', additional_kwargs={}, response_metadata={}, id='85236de7-930c-4ae7-8f6f-1a53ac919b06'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'ca8v3w38w', 'function': {'arguments': '{\"query\":\"Reinforcement Learning With Verifiable Rewards Paper\",\"search_depth\":\"advanced\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 1801, 'total_tokens': 1838, 'completion_time': 0.166126516, 'prompt_time': 0.251867906, 'queue_time': 0.088639702, 'total_time': 0.417994422}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3f0b524e-9cb9-40d5-96a5-b1583c1ebbad-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Reinforcement Learning With Verifiable Rewards Paper', 'search_depth': 'advanced', 'topic': 'general'}, 'id': 'ca8v3w38w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1801, 'output_tokens': 37, 'total_tokens': 1838}),\n",
       "  ToolMessage(content='{\"query\": \"Reinforcement Learning With Verifiable Rewards Paper\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://labelstud.io/blog/reinforcement-learning-from-verifiable-rewards/\", \"title\": \"Reinforcement Learning from Verifiable Rewards - Label Studio\", \"content\": \"# Reinforcement Learning from Verifiable Rewards\\\\n\\\\nReinforcement Learning with Verifiable Rewards is among the leading training strategies for injecting learning signals into LLMs, successfully employed by models such as DeepSeek R1 and Tülu 3. In a nutshell, Verifiable Rewards are simple functions that provide a clear-cut,  binary ground truth signal - typically a “1” (correct) or “0” (incorrect) - to indicate whether a model’s output meets a predefined correctness criterion. [...] Example: In the multi-turn code synthesis setup, verifiable rewards can be assigned based on the execution results of generated code against test cases. The reward function evaluates correctness at the end of each episode as following:\\\\n\\\\nFor more on the code execution example, see this 2024 paper.\\\\n\\\\n### Instruction-Following and Formatting Rewards [...] Verifiable rewards provide a quick and easy way to design robust RL environments, allowing subject matter experts to establish clear correctness criteria without deep machine learning expertise. Their explicit nature facilitates automated evaluation, minimizing reliance on human judgment and ensuring efficient and scalable integration into reinforcement learning pipelines.\\\\n\\\\n#### Robustness Against Reward Hacking\", \"score\": 0.84246206, \"raw_content\": null}, {\"url\": \"https://arxiv.org/abs/2506.14245\", \"title\": \"[2506.14245] Reinforcement Learning with Verifiable Rewards ...\", \"content\": \"|  |  |\\\\n| --- | --- |\\\\n| Comments: | Preprint |\\\\n| Subjects: | Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |\\\\n| Cite as: | arXiv:2506.14245 [cs.AI] |\\\\n|  | (or  arXiv:2506.14245v1 [cs.AI] for this version) |\\\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite (pending registration) |\\\\n\\\\n## Submission history\\\\n\\\\n## Access Paper:\\\\n\\\\nlicense icon\\\\n\\\\n### References & Citations\\\\n\\\\n## BibTeX formatted citation\\\\n\\\\n### Bookmark\\\\n\\\\nBibSonomy logo\\\\nReddit logo\\\\n\\\\n# Bibliographic and Citation Tools [...] Cornell University\\\\narxiv logo\\\\n\\\\nHelp | Advanced Search\\\\n\\\\narXiv logo\\\\nCornell University Logo\\\\n\\\\n## quick links\\\\n\\\\n# Computer Science > Artificial Intelligence\\\\n\\\\n# Title:Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs [...] # Code, Data and Media Associated with this Article\\\\n\\\\n# Demos\\\\n\\\\n# Recommenders and Search Tools\\\\n\\\\n# arXivLabs: experimental projects with community collaborators\\\\n\\\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", \"score\": 0.8131201, \"raw_content\": null}, {\"url\": \"https://arxiv.org/abs/2503.23829\", \"title\": \"Expanding RL with Verifiable Rewards Across Diverse Domains\", \"content\": \"Cornell University\\\\narxiv logo\\\\n\\\\nHelp | Advanced Search\\\\n\\\\narXiv logo\\\\nCornell University Logo\\\\n\\\\n## quick links\\\\n\\\\n# Computer Science > Computation and Language\\\\n\\\\n# Title:Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains\\\\n\\\\n|  |  |\\\\n| --- | --- |\\\\n| Subjects: | Computation and Language (cs.CL) |\\\\n| Cite as: | arXiv:2503.23829 [cs.CL] |\\\\n|  | (or  arXiv:2503.23829v2 [cs.CL] for this version) |\\\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite |\\\\n\\\\n## Submission history [...] ## Access Paper:\\\\n\\\\n### References & Citations\\\\n\\\\n## BibTeX formatted citation\\\\n\\\\n### Bookmark\\\\n\\\\nBibSonomy logo\\\\nReddit logo\\\\n\\\\n# Bibliographic and Citation Tools\\\\n\\\\n# Code, Data and Media Associated with this Article\\\\n\\\\n# Demos\\\\n\\\\n# Recommenders and Search Tools\\\\n\\\\n# arXivLabs: experimental projects with community collaborators\\\\n\\\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", \"score\": 0.57006866, \"raw_content\": null}], \"response_time\": 1.44}', name='tavily_search', id='96bc2e2b-ab4e-432d-963a-fa4702a546eb', tool_call_id='ca8v3w38w'),\n",
       "  AIMessage(content='Reinforcement Learning with Verifiable Rewards is a training strategy for Large Language Models (LLMs) that uses simple functions to provide a clear-cut, binary ground truth signal to indicate whether a model\\'s output meets a predefined correctness criterion. This approach has been successfully employed by models such as DeepSeek R1 and Tülu 3. Verifiable rewards can be assigned based on the execution results of generated code against test cases, and they provide a quick and easy way to design robust RL environments, allowing subject matter experts to establish clear correctness criteria without deep machine learning expertise. The explicit nature of verifiable rewards facilitates automated evaluation, minimizing reliance on human judgment and ensuring efficient and scalable integration into reinforcement learning pipelines. \\n\\nSome papers related to this topic include \"Reinforcement Learning with Verifiable Rewards\" and \"Expanding RL with Verifiable Rewards Across Diverse Domains\", which can be found on arXiv. These papers discuss the use of verifiable rewards in reinforcement learning and their potential to incentivize correct reasoning in base LLMs. \\n\\nOverall, Reinforcement Learning with Verifiable Rewards is a promising approach for training LLMs, and it has the potential to improve the accuracy and reliability of these models.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 246, 'prompt_tokens': 2854, 'total_tokens': 3100, 'completion_time': 0.65291357, 'prompt_time': 0.218793838, 'queue_time': 0.088126686, 'total_time': 0.871707408}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--634462dc-876b-43cc-b770-26e972c976d7-0', usage_metadata={'input_tokens': 2854, 'output_tokens': 246, 'total_tokens': 3100})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a29951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do you know Reinforcement Learning With Verifiable Rewards Paper, explain it?', additional_kwargs={}, response_metadata={}, id='85236de7-930c-4ae7-8f6f-1a53ac919b06'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'ca8v3w38w', 'function': {'arguments': '{\"query\":\"Reinforcement Learning With Verifiable Rewards Paper\",\"search_depth\":\"advanced\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 1801, 'total_tokens': 1838, 'completion_time': 0.166126516, 'prompt_time': 0.251867906, 'queue_time': 0.088639702, 'total_time': 0.417994422}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3f0b524e-9cb9-40d5-96a5-b1583c1ebbad-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Reinforcement Learning With Verifiable Rewards Paper', 'search_depth': 'advanced', 'topic': 'general'}, 'id': 'ca8v3w38w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1801, 'output_tokens': 37, 'total_tokens': 1838}),\n",
       " ToolMessage(content='{\"query\": \"Reinforcement Learning With Verifiable Rewards Paper\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://labelstud.io/blog/reinforcement-learning-from-verifiable-rewards/\", \"title\": \"Reinforcement Learning from Verifiable Rewards - Label Studio\", \"content\": \"# Reinforcement Learning from Verifiable Rewards\\\\n\\\\nReinforcement Learning with Verifiable Rewards is among the leading training strategies for injecting learning signals into LLMs, successfully employed by models such as DeepSeek R1 and Tülu 3. In a nutshell, Verifiable Rewards are simple functions that provide a clear-cut,  binary ground truth signal - typically a “1” (correct) or “0” (incorrect) - to indicate whether a model’s output meets a predefined correctness criterion. [...] Example: In the multi-turn code synthesis setup, verifiable rewards can be assigned based on the execution results of generated code against test cases. The reward function evaluates correctness at the end of each episode as following:\\\\n\\\\nFor more on the code execution example, see this 2024 paper.\\\\n\\\\n### Instruction-Following and Formatting Rewards [...] Verifiable rewards provide a quick and easy way to design robust RL environments, allowing subject matter experts to establish clear correctness criteria without deep machine learning expertise. Their explicit nature facilitates automated evaluation, minimizing reliance on human judgment and ensuring efficient and scalable integration into reinforcement learning pipelines.\\\\n\\\\n#### Robustness Against Reward Hacking\", \"score\": 0.84246206, \"raw_content\": null}, {\"url\": \"https://arxiv.org/abs/2506.14245\", \"title\": \"[2506.14245] Reinforcement Learning with Verifiable Rewards ...\", \"content\": \"|  |  |\\\\n| --- | --- |\\\\n| Comments: | Preprint |\\\\n| Subjects: | Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |\\\\n| Cite as: | arXiv:2506.14245 [cs.AI] |\\\\n|  | (or  arXiv:2506.14245v1 [cs.AI] for this version) |\\\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite (pending registration) |\\\\n\\\\n## Submission history\\\\n\\\\n## Access Paper:\\\\n\\\\nlicense icon\\\\n\\\\n### References & Citations\\\\n\\\\n## BibTeX formatted citation\\\\n\\\\n### Bookmark\\\\n\\\\nBibSonomy logo\\\\nReddit logo\\\\n\\\\n# Bibliographic and Citation Tools [...] Cornell University\\\\narxiv logo\\\\n\\\\nHelp | Advanced Search\\\\n\\\\narXiv logo\\\\nCornell University Logo\\\\n\\\\n## quick links\\\\n\\\\n# Computer Science > Artificial Intelligence\\\\n\\\\n# Title:Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs [...] # Code, Data and Media Associated with this Article\\\\n\\\\n# Demos\\\\n\\\\n# Recommenders and Search Tools\\\\n\\\\n# arXivLabs: experimental projects with community collaborators\\\\n\\\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", \"score\": 0.8131201, \"raw_content\": null}, {\"url\": \"https://arxiv.org/abs/2503.23829\", \"title\": \"Expanding RL with Verifiable Rewards Across Diverse Domains\", \"content\": \"Cornell University\\\\narxiv logo\\\\n\\\\nHelp | Advanced Search\\\\n\\\\narXiv logo\\\\nCornell University Logo\\\\n\\\\n## quick links\\\\n\\\\n# Computer Science > Computation and Language\\\\n\\\\n# Title:Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains\\\\n\\\\n|  |  |\\\\n| --- | --- |\\\\n| Subjects: | Computation and Language (cs.CL) |\\\\n| Cite as: | arXiv:2503.23829 [cs.CL] |\\\\n|  | (or  arXiv:2503.23829v2 [cs.CL] for this version) |\\\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite |\\\\n\\\\n## Submission history [...] ## Access Paper:\\\\n\\\\n### References & Citations\\\\n\\\\n## BibTeX formatted citation\\\\n\\\\n### Bookmark\\\\n\\\\nBibSonomy logo\\\\nReddit logo\\\\n\\\\n# Bibliographic and Citation Tools\\\\n\\\\n# Code, Data and Media Associated with this Article\\\\n\\\\n# Demos\\\\n\\\\n# Recommenders and Search Tools\\\\n\\\\n# arXivLabs: experimental projects with community collaborators\\\\n\\\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", \"score\": 0.57006866, \"raw_content\": null}], \"response_time\": 1.44}', name='tavily_search', id='96bc2e2b-ab4e-432d-963a-fa4702a546eb', tool_call_id='ca8v3w38w'),\n",
       " AIMessage(content='Reinforcement Learning with Verifiable Rewards is a training strategy for Large Language Models (LLMs) that uses simple functions to provide a clear-cut, binary ground truth signal to indicate whether a model\\'s output meets a predefined correctness criterion. This approach has been successfully employed by models such as DeepSeek R1 and Tülu 3. Verifiable rewards can be assigned based on the execution results of generated code against test cases, and they provide a quick and easy way to design robust RL environments, allowing subject matter experts to establish clear correctness criteria without deep machine learning expertise. The explicit nature of verifiable rewards facilitates automated evaluation, minimizing reliance on human judgment and ensuring efficient and scalable integration into reinforcement learning pipelines. \\n\\nSome papers related to this topic include \"Reinforcement Learning with Verifiable Rewards\" and \"Expanding RL with Verifiable Rewards Across Diverse Domains\", which can be found on arXiv. These papers discuss the use of verifiable rewards in reinforcement learning and their potential to incentivize correct reasoning in base LLMs. \\n\\nOverall, Reinforcement Learning with Verifiable Rewards is a promising approach for training LLMs, and it has the potential to improve the accuracy and reliability of these models.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 246, 'prompt_tokens': 2854, 'total_tokens': 3100, 'completion_time': 0.65291357, 'prompt_time': 0.218793838, 'queue_time': 0.088126686, 'total_time': 0.871707408}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--634462dc-876b-43cc-b770-26e972c976d7-0', usage_metadata={'input_tokens': 2854, 'output_tokens': 246, 'total_tokens': 3100})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359bd9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement Learning with Verifiable Rewards is a training strategy for Large Language Models (LLMs) that uses simple functions to provide a clear-cut, binary ground truth signal to indicate whether a model's output meets a predefined correctness criterion. This approach has been successfully employed by models such as DeepSeek R1 and Tülu 3. Verifiable rewards can be assigned based on the execution results of generated code against test cases, and they provide a quick and easy way to design robust RL environments, allowing subject matter experts to establish clear correctness criteria without deep machine learning expertise. The explicit nature of verifiable rewards facilitates automated evaluation, minimizing reliance on human judgment and ensuring efficient and scalable integration into reinforcement learning pipelines. \n",
      "\n",
      "Some papers related to this topic include \"Reinforcement Learning with Verifiable Rewards\" and \"Expanding RL with Verifiable Rewards Across Diverse Domains\", which can be found on arXiv. These papers discuss the use of verifiable rewards in reinforcement learning and their potential to incentivize correct reasoning in base LLMs. \n",
      "\n",
      "Overall, Reinforcement Learning with Verifiable Rewards is a promising approach for training LLMs, and it has the potential to improve the accuracy and reliability of these models.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f12df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '4nsxrf79v', 'function': {'arguments': '{\"query\":\"newest top reinforcement learning papers\",\"search_depth\":\"advanced\",\"time_range\":\"month\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 1797, 'total_tokens': 1838, 'completion_time': 0.125490664, 'prompt_time': 0.14009176, 'queue_time': 0.085270255, 'total_time': 0.265582424}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--08919fb6-b10d-426a-ba7a-4d884aa7bbb9-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'newest top reinforcement learning papers', 'search_depth': 'advanced', 'time_range': 'month', 'topic': 'general'}, 'id': '4nsxrf79v', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1797, 'output_tokens': 41, 'total_tokens': 1838})]}}\n",
      "\n",
      "===========================================\n",
      "\n",
      "{'tools': {'messages': [ToolMessage(content='{\"query\": \"newest top reinforcement learning papers\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://arxiv.org/list/stat.ML/recent\", \"title\": \"Machine Learning - arXiv\", \"content\": \"Subjects:Data Structures and Algorithms (cs.DS); Numerical Analysis (math.NA); Optimization and Control (math.OC); Machine Learning (stat.ML)  Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, Joe Benton, Joseph Bloom, Mark Chen, Alan Cooney, Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Ryan Greenblatt, Dan Hendrycks, Marius Hobbhahn, Evan Hubinger, Geoffrey Irving, Erik Jenner, Daniel Kokotajlo, Victoria Krakovna, Shane Legg, David Lindner, David Luan, Aleksander Mądry, Julian Michael, Neel Nanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary Phuong, Fabien Roger, Joshua Saxe, Buck Shlegeris, Martín Soto, Eric Steinberger, Jasmine Wang, Wojciech Zaremba, Bowen Baker, Rohin Shah, Vlad Mikulik Subjects:Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)  Subjects:Machine Learning (stat.ML); Machine Learning (cs.LG); Dynamical Systems (math.DS); Data Analysis, Statistics and Probability (physics.data-an) \", \"score\": 0.98556, \"raw_content\": null}, {\"url\": \"http://www.arxiv.org/abs/2507.22804\", \"title\": \"[2507.22804] Deep reinforcement learning for efficient exploration of ...\", \"content\": \"This paper proposes a reinforcement learning framework for performance-driven structural design that combines bottom-up design generation with learned\", \"score\": 0.98529, \"raw_content\": null}, {\"url\": \"https://machinelearning.apple.com/research/icml-2025\", \"title\": \"Apple Machine Learning Research at ICML 2025\", \"content\": \"At the main conference and associated workshops, Apple researchers will present new research across a number of topics in AI and ML, including advances in computer vision, language models, diffusion models, reinforcement learning, and more. In an oral presentation at ICML, Apple researchers will share Normalizing Flows are Capable Generative Models, which shows that NFs are more powerful than previously believed and can be used for high-quality image generation. At ICML, Apple researchers will present Mechanisms of Projective Composition of Diffusion Models, a new paper that studies the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and length-generalization. At ICML, Apple researchers will present Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging, which addresses this challenge.\", \"score\": 0.97502, \"raw_content\": null}], \"response_time\": 4.05}', name='tavily_search', id='c903e421-fb14-4199-a302-470bf755aba8', tool_call_id='4nsxrf79v')]}}\n",
      "\n",
      "===========================================\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content='Based on the search results, the newest and top reinforcement learning papers can be found on arXiv, a popular online repository of electronic preprints in physics, mathematics, computer science, and related disciplines. The search results include papers on various topics in reinforcement learning, such as deep reinforcement learning, structural design, and diffusion models.\\n\\nSome of the top papers in reinforcement learning include:\\n\\n* \"Deep reinforcement learning for efficient exploration of structural design spaces\" (arXiv:2507.22804)\\n* \"Normalizing Flows are Capable Generative Models\" (Apple Machine Learning Research at ICML 2025)\\n* \"Mechanisms of Projective Composition of Diffusion Models\" (Apple Machine Learning Research at ICML 2025)\\n* \"Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging\" (Apple Machine Learning Research at ICML 2025)\\n\\nThese papers present new research and advances in reinforcement learning, including the use of deep learning techniques, diffusion models, and specialist models. They can be found on arXiv and other online repositories, and are relevant to researchers and practitioners in the field of reinforcement learning.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 234, 'prompt_tokens': 2508, 'total_tokens': 2742, 'completion_time': 0.559810813, 'prompt_time': 0.188154086, 'queue_time': 0.677235468, 'total_time': 0.747964899}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--514821ad-848d-4917-9521-fe9034dd2184-0', usage_metadata={'input_tokens': 2508, 'output_tokens': 234, 'total_tokens': 2742})]}}\n",
      "\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the newest and Top Reinforcement Learning Papers?\")]}\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n===========================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037afcdb",
   "metadata": {},
   "source": [
    "### Adding memory\n",
    "- Adding memory in LangGraph is very similar to what we did with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3244f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb870197",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent_exec = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64b3d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e42b0682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '8brz0waq2', 'function': {'arguments': '{\"query\":\"newest top reinforcement learning papers\",\"search_depth\":\"advanced\",\"time_range\":\"month\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 1797, 'total_tokens': 1838, 'completion_time': 0.084866109, 'prompt_time': 0.138013123, 'queue_time': 0.085389003, 'total_time': 0.222879232}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ecf5acea-cdae-484e-bd74-08c872b2f683-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'newest top reinforcement learning papers', 'search_depth': 'advanced', 'time_range': 'month', 'topic': 'general'}, 'id': '8brz0waq2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1797, 'output_tokens': 41, 'total_tokens': 1838})]}}\n",
      "\n",
      "===========================================\n",
      "\n",
      "{'tools': {'messages': [ToolMessage(content='{\"query\": \"newest top reinforcement learning papers\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://arxiv.org/list/stat.ML/recent\", \"title\": \"Machine Learning - arXiv\", \"content\": \"Subjects:Data Structures and Algorithms (cs.DS); Numerical Analysis (math.NA); Optimization and Control (math.OC); Machine Learning (stat.ML)  Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, Joe Benton, Joseph Bloom, Mark Chen, Alan Cooney, Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Ryan Greenblatt, Dan Hendrycks, Marius Hobbhahn, Evan Hubinger, Geoffrey Irving, Erik Jenner, Daniel Kokotajlo, Victoria Krakovna, Shane Legg, David Lindner, David Luan, Aleksander Mądry, Julian Michael, Neel Nanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary Phuong, Fabien Roger, Joshua Saxe, Buck Shlegeris, Martín Soto, Eric Steinberger, Jasmine Wang, Wojciech Zaremba, Bowen Baker, Rohin Shah, Vlad Mikulik Subjects:Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)  Subjects:Machine Learning (stat.ML); Machine Learning (cs.LG); Dynamical Systems (math.DS); Data Analysis, Statistics and Probability (physics.data-an) \", \"score\": 0.98556, \"raw_content\": null}, {\"url\": \"http://www.arxiv.org/abs/2507.22804\", \"title\": \"[2507.22804] Deep reinforcement learning for efficient exploration of ...\", \"content\": \"This paper proposes a reinforcement learning framework for performance-driven structural design that combines bottom-up design generation with learned\", \"score\": 0.98529, \"raw_content\": null}, {\"url\": \"https://machinelearning.apple.com/research/icml-2025\", \"title\": \"Apple Machine Learning Research at ICML 2025\", \"content\": \"At the main conference and associated workshops, Apple researchers will present new research across a number of topics in AI and ML, including advances in computer vision, language models, diffusion models, reinforcement learning, and more. In an oral presentation at ICML, Apple researchers will share Normalizing Flows are Capable Generative Models, which shows that NFs are more powerful than previously believed and can be used for high-quality image generation. At ICML, Apple researchers will present Mechanisms of Projective Composition of Diffusion Models, a new paper that studies the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and length-generalization. At ICML, Apple researchers will present Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging, which addresses this challenge.\", \"score\": 0.97502, \"raw_content\": null}], \"response_time\": 1.22}', name='tavily_search', id='fdc70f11-e0fc-46bf-8c2e-f9967f04bd44', tool_call_id='8brz0waq2')]}}\n",
      "\n",
      "===========================================\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content='Based on the search results, the newest and top reinforcement learning papers can be found on arXiv, a popular platform for publishing and sharing research papers. Some of the top papers in the field of reinforcement learning include:\\n\\n* \"Deep Reinforcement Learning for Efficient Exploration of Structural Design Spaces\" (arXiv:2507.22804)\\n* \"Normalizing Flows are Capable Generative Models\" (Apple Machine Learning Research at ICML 2025)\\n* \"Mechanisms of Projective Composition of Diffusion Models\" (Apple Machine Learning Research at ICML 2025)\\n* \"Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging\" (Apple Machine Learning Research at ICML 2025)\\n\\nThese papers present new research and advances in reinforcement learning, including efficient exploration of structural design spaces, generative models, diffusion models, and specialist models. They can be found on arXiv and other research platforms, and are considered to be among the top papers in the field of reinforcement learning.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 209, 'prompt_tokens': 2508, 'total_tokens': 2717, 'completion_time': 0.496439962, 'prompt_time': 0.189081776, 'queue_time': 0.086015574, 'total_time': 0.685521738}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--fe41d518-778c-4669-8ea3-d685517df0fc-0', usage_metadata={'input_tokens': 2508, 'output_tokens': 209, 'total_tokens': 2717})]}}\n",
      "\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the newest and Top Reinforcement Learning Papers?\")]},\n",
    "    config=config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n===========================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56bff7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'dtcsprqjj', 'function': {'arguments': '{\"query\":\"RLVR Paper\",\"search_depth\":\"advanced\",\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 1793, 'total_tokens': 1824, 'completion_time': 0.077799126, 'prompt_time': 0.137513541, 'queue_time': 0.088205402, 'total_time': 0.215312667}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--09c3a4f8-9cb8-489d-85ad-9efa3c2beeb5-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'RLVR Paper', 'search_depth': 'advanced', 'topic': 'general'}, 'id': 'dtcsprqjj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1793, 'output_tokens': 31, 'total_tokens': 1824})]}}\n",
      "\n",
      "===========================================\n",
      "\n",
      "{'tools': {'messages': [ToolMessage(content='{\"query\": \"RLVR Paper\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://bullishlemon.substack.com/p/reinforcement-learning-with-verifiable\", \"title\": \"Reinforcement Learning (with Verifiable Rewards) Doesn\\'t ...\", \"content\": \"Yesterday, I was doing some searching n’ thinking about AI R&D automation (as per my last post) when I stumbled upon an absolute madlad of a paper. This paper, fresh off the press (published two weeks ago), claims that Reinforcement Learning with Verifiable Rewards (RLVR)–the method that made o3–_does not_elicit fundamentally new capabilities from the base model. Huge if true. [...] Like Toner’s post, the paper begins with the premise that RLVR has been used to elicit reasoning behaviors not present in base models (e.g. enumeration, self-reflection, iterative refinement). But then, the authors proceed to courageously ask: did it, like,_actually_, though? [...] It was linked in a comment on Helen Toner’s excellent recent post on verifiable rewards, generalization, and scaling reasoning training. I’ll cover her post first for context, then come back to the paper.\\\\n\\\\nToner’s Post\\\\n============\\\\n\\\\nThe post lays out how the shiny new reasoning models (e.g. OAI o1, DeepSeek-r1, Gemini Flash Thinking) were developed through post-training using RLVR, which utilizes cheap and scalable automated tests, rather than RLHF’s costly human feedback.\", \"score\": 0.75268626, \"raw_content\": null}, {\"url\": \"https://huggingface.co/papers/2506.14245\", \"title\": \"Paper page - Reinforcement Learning with Verifiable Rewards ...\", \"content\": \"We present a theoretical framework and empirical evidence demonstrating that reinforcement learning with verifiable rewards (RLVR) implicitly incentivizes correct reasoning in large language models (LLMs). This insight resolves a key debate in the field: _whether RLVR-driven improvements extend beyond the inherent capabilities of base LLMs_. While prevailing assumptions attribute gains in Pass@1 solely to the original Pass@K performance of pretrained models, our findings reveal that RLVR [...] results are supportive: using CoT-Pass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning. [...] Mao Yang \\\\n\\\\nAbstract\\\\n--------\\\\n\\\\nRLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.\\\\n\\\\n AI-generated summary\", \"score\": 0.71088976, \"raw_content\": null}, {\"url\": \"https://arxiv.org/abs/2504.20571\", \"title\": \"Reinforcement Learning for Reasoning in Large Language Models ...\", \"content\": \"> Abstract:We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the mathematical reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the [...] View a PDF of the paper titled Reinforcement Learning for Reasoning in Large Language Models with One Training Example, by Yiping Wang and 13 other authors [...] Title:Reinforcement Learning for Reasoning in Large Language Models with One Training Example\\\\n=============================================================================================\\\\n\\\\nAuthors:Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen\", \"score\": 0.61972505, \"raw_content\": null}], \"response_time\": 1.79}', name='tavily_search', id='9d3a342d-fc72-44b1-90ec-1da9a4f63027', tool_call_id='dtcsprqjj')]}}\n",
      "\n",
      "===========================================\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content='The RLVR Paper refers to a research paper that discusses the concept of Reinforcement Learning with Verifiable Rewards (RLVR) and its application in large language models. The paper presents a theoretical framework and empirical evidence demonstrating that RLVR implicitly incentivizes correct reasoning in large language models, resolving a key debate in the field. The paper also provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 2762, 'total_tokens': 2860, 'completion_time': 0.211727621, 'prompt_time': 0.212046345, 'queue_time': 0.089373664, 'total_time': 0.423773966}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7c76777a-e17e-4bb4-b60d-8feb64bb587b-0', usage_metadata={'input_tokens': 2762, 'output_tokens': 98, 'total_tokens': 2860})]}}\n",
      "\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the RLVR Paper?\")]},\n",
    "    config=config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n===========================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e3e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
